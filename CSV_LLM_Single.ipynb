{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkknnaO4mICqe/FUIgHXfZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirtibala/Machine-learning/blob/main/CSV_LLM_Single.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wszmYeKePQw2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain langchain-openai langchain-community langchain-experimental pandas"
      ],
      "metadata": {
        "id": "ckiXBfUVPXGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90800a91-0ea9-4f13-aef2-10df8a699c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.25.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "yk1UXALGupTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "df = pd.read_csv(\"/content/insurance_data.csv\")"
      ],
      "metadata": {
        "id": "jJxnB8OWvZ8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"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\"\n"
      ],
      "metadata": {
        "id": "UVk_RG-Tzon8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_pandas_dataframe_agent(OpenAI(temperature=0),df, verbose=True)"
      ],
      "metadata": {
        "id": "47f4C1S4wKQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_pandas_dataframe_agent(\n",
        "    ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\"),\n",
        "    df,\n",
        "    verbose=True,\n",
        "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
        ")"
      ],
      "metadata": {
        "id": "0M_HYHLf0RuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"how many rows are there?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "Hfzi9aXM0TEm",
        "outputId": "f354cc97-8e09-4f11-ec0b-a8bad4825e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': 'df.shape[0]'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m250\u001b[0m\u001b[32;1m\u001b[1;3mThere are 250 rows in the dataframe.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are 250 rows in the dataframe.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"how many features are there?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "m7xlPj5N0XSl",
        "outputId": "30f7ef2d-f445-449e-8a61-506600a09a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': 'len(df.columns)'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m15\u001b[0m\u001b[32;1m\u001b[1;3mThere are 15 features in the dataframe.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are 15 features in the dataframe.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"what is the count of fraud detected?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "BjfMxqEh1OKy",
        "outputId": "ba27bbc3-9f38-4de8-c719-6f0090f6b259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"df['Anomaly Flag'].sum()\"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m109.0\u001b[0m\u001b[32;1m\u001b[1;3mThe count of fraud detected is 109.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The count of fraud detected is 109.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"Give Name or ID of first five fraudlents?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "-b1C0D0Q1ZNI",
        "outputId": "6dee8aeb-717e-4822-a4f0-efb5352c31fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"df[df['Anomaly Flag'] == 1]['Policy Number'].head().tolist()\"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m['P134', 'P469', 'P362', 'P962', 'P923']\u001b[0m\u001b[32;1m\u001b[1;3mThe names or IDs of the first five fraudulent policies are: P134, P469, P362, P962, P923.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The names or IDs of the first five fraudulent policies are: P134, P469, P362, P962, P923.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"Give branch name of first five fraudlents?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "mFR9nR1X119f",
        "outputId": "516fe66f-1b9f-4862-9d2f-c5e010341517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `python_repl_ast` with `{'query': \"df[df['Anomaly Flag'] == 1]['Branch'].head().tolist()\"}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m['B184', 'B964', 'B724', 'B510', 'B956']\u001b[0m\u001b[32;1m\u001b[1;3mThe branch names of the first five fraudulents are B184, B964, B724, B510, and B956.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The branch names of the first five fraudulents are B184, B964, B724, B510, and B956.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"Give the feature name which are most responsible to detect the fraudlents?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "Y7m8NoNs2K2R",
        "outputId": "a3ed42d6-81fa-41d6-ebd2-2d1c2d1359cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo determine the features that are most responsible for detecting fraud, you can use various techniques such as feature importance or correlation analysis. Here are a few approaches you can try:\n",
            "\n",
            "1. Random Forest Classifier: Train a random forest classifier on your dataset and use the feature_importances_ attribute to get the importance scores of each feature. Higher importance scores indicate that the feature is more responsible for detecting fraud.\n",
            "\n",
            "2. XGBoost Classifier: Train an XGBoost classifier and use the feature_importances_ attribute to get the importance scores of each feature. Similar to random forest, higher importance scores indicate more responsibility in detecting fraud.\n",
            "\n",
            "3. Correlation Analysis: Calculate the correlation matrix of your dataset and identify the features that have a higher correlation with the target variable (fraud). Features with higher correlation values are more likely to be responsible for detecting fraud.\n",
            "\n",
            "4. Univariate Feature Selection: Use statistical tests like chi-square or ANOVA to select the features that have the strongest relationship with the target variable. These features are more likely to be responsible for detecting fraud.\n",
            "\n",
            "Remember that the choice of the most appropriate method depends on your specific dataset and problem. It's recommended to try multiple approaches and compare the results to get a more comprehensive understanding of the responsible features for fraud detection.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"To determine the features that are most responsible for detecting fraud, you can use various techniques such as feature importance or correlation analysis. Here are a few approaches you can try:\\n\\n1. Random Forest Classifier: Train a random forest classifier on your dataset and use the feature_importances_ attribute to get the importance scores of each feature. Higher importance scores indicate that the feature is more responsible for detecting fraud.\\n\\n2. XGBoost Classifier: Train an XGBoost classifier and use the feature_importances_ attribute to get the importance scores of each feature. Similar to random forest, higher importance scores indicate more responsibility in detecting fraud.\\n\\n3. Correlation Analysis: Calculate the correlation matrix of your dataset and identify the features that have a higher correlation with the target variable (fraud). Features with higher correlation values are more likely to be responsible for detecting fraud.\\n\\n4. Univariate Feature Selection: Use statistical tests like chi-square or ANOVA to select the features that have the strongest relationship with the target variable. These features are more likely to be responsible for detecting fraud.\\n\\nRemember that the choice of the most appropriate method depends on your specific dataset and problem. It's recommended to try multiple approaches and compare the results to get a more comprehensive understanding of the responsible features for fraud detection.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"Give the feature name which are most responsible to detect the fraudlents from the given dataset?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qqEmgkWR6NkX",
        "outputId": "5df8a1d3-733c-4f30-9575-494aed48b60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo determine the features that are most responsible for detecting fraud in the given dataset, we can use a machine learning algorithm to identify the most important features. One commonly used algorithm for feature importance is the Random Forest algorithm.\n",
            "\n",
            "Here's how you can use the Random Forest algorithm to determine feature importance in Python:\n",
            "\n",
            "```python\n",
            "import pandas as pd\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "\n",
            "# Load the dataset into a pandas dataframe\n",
            "df = pd.read_csv('your_dataset.csv')\n",
            "\n",
            "# Drop any rows with missing values\n",
            "df = df.dropna()\n",
            "\n",
            "# Separate the features (X) and the target variable (y)\n",
            "X = df.drop('Anomaly Flag', axis=1)\n",
            "y = df['Anomaly Flag']\n",
            "\n",
            "# Create a Random Forest classifier\n",
            "rf = RandomForestClassifier()\n",
            "\n",
            "# Fit the classifier to the data\n",
            "rf.fit(X, y)\n",
            "\n",
            "# Get the feature importances\n",
            "importances = rf.feature_importances_\n",
            "\n",
            "# Create a dataframe to store the feature importances\n",
            "feature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
            "\n",
            "# Sort the dataframe by importance in descending order\n",
            "feature_importances = feature_importances.sort_values('Importance', ascending=False)\n",
            "\n",
            "# Print the top 5 most important features\n",
            "print(feature_importances.head(5))\n",
            "```\n",
            "\n",
            "This code will fit a Random Forest classifier to the dataset and calculate the feature importances. The resulting dataframe will show the features ranked by importance, with the most important features at the top.\n",
            "\n",
            "Please note that the code assumes that the target variable is named 'Anomaly Flag' and that all the features are numerical. If your dataset has categorical features, you may need to encode them before using the Random Forest algorithm.\n",
            "\n",
            "You can replace 'your_dataset.csv' with the actual file path or provide the dataframe directly if it is already loaded.\n",
            "\n",
            "By examining the top-ranked features, you can identify the ones that are most responsible for detecting fraud in the dataset.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"To determine the features that are most responsible for detecting fraud in the given dataset, we can use a machine learning algorithm to identify the most important features. One commonly used algorithm for feature importance is the Random Forest algorithm.\\n\\nHere's how you can use the Random Forest algorithm to determine feature importance in Python:\\n\\n```python\\nimport pandas as pd\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\n# Load the dataset into a pandas dataframe\\ndf = pd.read_csv('your_dataset.csv')\\n\\n# Drop any rows with missing values\\ndf = df.dropna()\\n\\n# Separate the features (X) and the target variable (y)\\nX = df.drop('Anomaly Flag', axis=1)\\ny = df['Anomaly Flag']\\n\\n# Create a Random Forest classifier\\nrf = RandomForestClassifier()\\n\\n# Fit the classifier to the data\\nrf.fit(X, y)\\n\\n# Get the feature importances\\nimportances = rf.feature_importances_\\n\\n# Create a dataframe to store the feature importances\\nfeature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\\n\\n# Sort the dataframe by importance in descending order\\nfeature_importances = feature_importances.sort_values('Importance', ascending=False)\\n\\n# Print the top 5 most important features\\nprint(feature_importances.head(5))\\n```\\n\\nThis code will fit a Random Forest classifier to the dataset and calculate the feature importances. The resulting dataframe will show the features ranked by importance, with the most important features at the top.\\n\\nPlease note that the code assumes that the target variable is named 'Anomaly Flag' and that all the features are numerical. If your dataset has categorical features, you may need to encode them before using the Random Forest algorithm.\\n\\nYou can replace 'your_dataset.csv' with the actual file path or provide the dataframe directly if it is already loaded.\\n\\nBy examining the top-ranked features, you can identify the ones that are most responsible for detecting fraud in the dataset.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KuC_oYiT6c4m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}